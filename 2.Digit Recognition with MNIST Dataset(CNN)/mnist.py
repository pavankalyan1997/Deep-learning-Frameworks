# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gkr83OIFwv7Dn0-E_B-q1BAYQvfq8jZH
"""

import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
import scipy
import random
import warnings
warnings.filterwarnings("ignore")

from PIL import Image
from scipy import ndimage
import tensorflow as tf
from tensorflow.python.framework import ops

from keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("Shape of Training Examples:"+str(X_train.shape))

print("Shape of Training Labels: "+str(y_train.shape))

print("Shape of Testing Examples: "+str(X_test.shape))

print("Shape of Testing Labels:"+str(y_test.shape))

#sample training image
plt.imshow(X_train[0])

#It's corresponding target
y_train[0]

# visualize first 15 images in a grid and their corresponding targets
images_and_labels=list(zip(X_train,y_train))
plt.figure(figsize=(5,5))
for index,(image,label) in enumerate(images_and_labels[:15]):
    plt.subplot(3,5,index+1)
    plt.axis('off')
    plt.imshow(image,cmap=plt.cm.gray_r,interpolation='nearest')
    plt.title('%i' % label)

#scale the images (feature scaling)
X_train=X_train/255
X_test=X_test/255
print(X_train[0])

print ("number of training examples = " + str(X_train.shape[0]))
print ("number of test examples = " + str(X_test.shape[0]))

print("Type of X_train is: "+ str(type(X_train)))
print("Type of y_train is: "+str(type(y_train)))
print("Type of X_test is: "+str(type(X_test)))
print("Type of y_test is: "+str(type(y_test)))

print("Number of unique training labels in y_train are : "+str(len(np.unique(y_train))))
print("Number of unique testing labels in y_test are : "+str(len(np.unique(y_test))))

# reshape each example in X_train and X_test into 3D ndarray
X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)
X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)
print("Shape of X_train is : "+str(X_train.shape))
print("Shape of X_test is : "+str(X_test.shape))

# reshape each example is y_train and y_test into Matrix 
y_train=y_train.reshape(y_train.shape[0],1)
y_test=y_test.reshape(y_test.shape[0],1)
print("Shape of y_train is : "+str(y_train.shape))
print("Shape of y_test is : "+str(y_test.shape))

def encode_Y(y):
  y_encoded=np.zeros((y.shape[0],10))
  for i in range(y_encoded.shape[0]):
      y_encoded[i][y[i][0]]=1
  return y_encoded

y_train=encode_Y(y_train)
y_test=encode_Y(y_test)
print("Shape of y_train is :"+str(y_train.shape))
print("Shape of y_test is :"+str(y_test.shape))
print("Sample training label :"+str(y_train[0]))

#create placeholders
def create_placeholders(n_H0,n_W0,n_C0,n_y):
    X=tf.placeholder(tf.float64,[None,n_H0,n_W0,n_C0])
    Y=tf.placeholder(tf.float64,[None,n_y])
    return X,Y

X_p,Y_p=create_placeholders(28,28,1,10)
print("X= "+ str(X_p))
print("Y= "+str(Y_p))

def initialize_parameters():
    tf.set_random_seed(1)
    W1=tf.get_variable("W1",[4,4,1,8],dtype=tf.float64,initializer=tf.contrib.layers.xavier_initializer(seed=0))
    W2=tf.get_variable("W2",[2,2,8,16],dtype=tf.float64,initializer=tf.contrib.layers.xavier_initializer(seed=0))
    parameters={"W1":W1,"W2":W2}
    return parameters

tf.reset_default_graph()
with tf.Session() as sess_test:
    parameters = initialize_parameters()
    init = tf.global_variables_initializer()
    sess_test.run(init)
    print("W1 = " + str(parameters["W1"].eval()[1,1]))
    print("W2 = " + str(parameters["W2"].eval()[1,1,1]))

def forward_propagation(X, parameters):
    W1 = parameters['W1']
    W2 = parameters['W2']
    Z1 = tf.nn.conv2d(X,W1,strides = [1,1,1,1], padding = 'SAME')
    A1 = tf.nn.relu(Z1)
    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')
    Z2 = tf.nn.conv2d(P1,W2,strides = [1,1,1,1], padding = 'SAME')
    A2 = tf.nn.relu(Z2)
    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')
    P2 = tf.contrib.layers.flatten(P2)
    Z3 = tf.contrib.layers.fully_connected(P2,10,activation_fn=None)
    return Z3

tf.reset_default_graph()
with tf.Session() as sess:
    np.random.seed(1)
    X_p,Y_p=create_placeholders(28,28,1,10)
    parameters = initialize_parameters()
    Z3 = forward_propagation(X_p, parameters)
    init = tf.global_variables_initializer()
    sess.run(init)
    a = sess.run(Z3, {X_p: X_train, Y_p: y_train})
    print("Z3 = " + str(a))

def compute_cost(Z3,Y):
  cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))
  return cost

tf.reset_default_graph()
with tf.Session() as sess:
    np.random.seed(1)
    X_p,Y_p=create_placeholders(28,28,1,10)
    parameters = initialize_parameters()
    Z3 = forward_propagation(X_p, parameters)
    cost = compute_cost(Z3, y_train)
    init = tf.global_variables_initializer()
    sess.run(init)
    a = sess.run(cost, {X_p: X_train, Y_p: y_train})
    print("cost = " + str(a))

def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    np.random.seed(seed)            
    m = X.shape[0]                  
    mini_batches = []
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation]
    shuffled_Y = Y[permutation]
    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size]
        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[(mini_batch_size*math.floor(m/mini_batch_size)):]
        mini_batch_Y = shuffled_Y[(mini_batch_size*math.floor(m/mini_batch_size)):]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches

minibatches=random_mini_batches(X_train,y_train,64,2)

minibatch=minibatches[0]
(minibatch_X,minibatch_Y)=minibatch
print(minibatch_X.shape)
print(minibatch_Y.shape)

def model(X_train,Y_train,X_test,Y_test,learning_rate=0.009,num_epochs=100,minibatch_size=64,print_cost=True):
  ops.reset_default_graph()
  tf.set_random_seed(1)
  seed=3
  #useful variables
  (m,n_H0,n_W0,n_C0)=X_train.shape
  n_y=Y_train.shape[1]
  costs=[]
  
  #create placeholders
  X,Y=create_placeholders(n_H0,n_W0,n_C0,n_y)
  
  #initialize parameters
  
  parameters=initialize_parameters()
  
  #forward propagation
  
  Z3= forward_propagation(X, parameters)
  
  # compute cost
  cost=compute_cost(Z3,Y)
  
  #Back proapagation
  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)
  
  init=tf.global_variables_initializer()
  
  with tf.Session() as sess:
    sess.run(init)
    for epoch in range(num_epochs):
      minibatch_cost=0
      num_minibatches=int(m/minibatch_size)
      seed=seed+1
      minibatches=random_mini_batches(X_train,Y_train,minibatch_size,seed)
      for minibatch in minibatches:
        (minibatch_X,minibatch_Y)=minibatch
        _,temp_cost=sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})
        minibatch_cost+=(temp_cost/num_minibatches)
      if print_cost==True and epoch%5==0:
        print ("Cost after epoch %i: %f" % (epoch, minibatch_cost))
      if print_cost==True and epoch%1==0:
        costs.append(minibatch_cost)
        
    #plot the cost
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per tens)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    # Calculate the correct predictions
    predict_op = tf.argmax(Z3, 1)
    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))

    # Calculate accuracy on the test set
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print(accuracy)
    train_accuracy = accuracy.eval({X: X_train, Y: Y_train})
    test_accuracy = accuracy.eval({X: X_test, Y: Y_test})
    print("Train Accuracy:", train_accuracy)
    print("Test Accuracy:", test_accuracy)
    
    
    
    return train_accuracy,test_accuracy,parameters

_, _, parameters = model(X_train, y_train, X_test, y_test)

